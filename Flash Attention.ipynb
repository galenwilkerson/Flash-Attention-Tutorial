{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98aa4b05",
   "metadata": {},
   "source": [
    "# Flash Attention:  Intuition\n",
    "\n",
    "\n",
    "### Understanding Modularity in Attention Matrices\n",
    "\n",
    "#### Why Attention Matrices Tend to Be Modular\n",
    "\n",
    "1. **Local Dependencies**\n",
    "   - **Natural Language Processing (NLP) Example**:\n",
    "     - **Sentence Structure**: In NLP, words in a sentence have strong dependencies on their neighboring words. For instance, in the phrase \"the quick brown fox,\" the word \"quick\" is more related to \"brown\" than to a distant word like \"jumps.\"\n",
    "     - **Local Attention**: Attention mechanisms often focus on these local dependencies, resulting in a block-diagonal structure where each word attends to its nearby words, forming modular blocks along the main diagonal of the attention matrix.\n",
    "\n",
    "2. **Hierarchical Structure**\n",
    "   - **Hierarchical Data**:\n",
    "     - **Document Structure**: Documents have a hierarchical structure, with sections, paragraphs, sentences, and words. Each level in this hierarchy tends to have internal dependencies that are stronger than those between different levels.\n",
    "     - **Hierarchical Attention**: In tasks like document classification, attention mechanisms can focus on specific sections or paragraphs, leading to modular blocks corresponding to these hierarchical levels.\n",
    "\n",
    "3. **Efficiency and Scalability**\n",
    "   - **Computational Constraints**:\n",
    "     - **Efficiency**: Processing entire sequences without considering modularity can be computationally expensive and memory-intensive. By focusing on smaller, more relevant sections of the sequence, attention mechanisms can be more efficient.\n",
    "     - **Scalability**: Modularity allows attention mechanisms to scale to longer sequences by reducing the complexity of the attention computation. This is particularly important in tasks involving long documents or time-series data.\n",
    "\n",
    "4. **Sparsity and Redundancy Reduction**\n",
    "   - **Sparse Interactions**:\n",
    "     - **Sparse Attention**: In many real-world applications, interactions are sparse, meaning that only a small subset of elements in the sequence interacts strongly with each other. This leads to a sparse attention matrix with non-zero values concentrated in specific blocks.\n",
    "     - **Redundancy Reduction**: By focusing attention on these sparse interactions, modularity helps reduce redundancy in computations, leading to more efficient processing.\n",
    "\n",
    "5. **Semantic Coherence**\n",
    "   - **Semantic Relationships**:\n",
    "     - **Coherent Blocks**: In tasks involving semantic relationships, such as image captioning or machine translation, certain parts of the input have strong internal coherence. For example, words describing a specific object in an image are likely to attend to each other.\n",
    "     - **Modular Attention**: This internal coherence results in modular blocks within the attention matrix, where each block captures the semantic relationships within a coherent subset of the input.\n",
    "\n",
    "#### Multi-Scale Attention and Stacking Layers\n",
    "\n",
    "- **Local Modularity at Initial Layers**: The initial layers of a transformer focus on capturing local dependencies, resulting in modular blocks at a small scale.\n",
    "- **Stacking Attention Layers**: As more layers are stacked, each subsequent layer builds on the representations from the previous layers, gradually capturing longer-range dependencies and more abstract relationships.\n",
    "- **Multi-Scale Representation**: Lower layers capture fine-grained details, while higher layers capture broader context, resulting in a multi-scale attention mechanism that combines local and global information.\n",
    "\n",
    "### Graphically Drawing Modular Matrices\n",
    "\n",
    "In LaTeX, we can represent modular matrices graphically using block matrix notation. Let's first illustrate a modular attention matrix and then discuss the computational complexity of efficient versus inefficient matrix multiplication.\n",
    "\n",
    "#### Modular Attention Matrix\n",
    "\n",
    "Consider a modular attention matrix for a sequence of length 12, divided into 3 blocks of size 4:\n",
    "\n",
    "$\n",
    "\\mathbf{A} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{A}_1 & 0 & 0 \\\\\n",
    "0 & \\mathbf{A}_2 & 0 \\\\\n",
    "0 & 0 & \\mathbf{A}_3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Here, $\\mathbf{A}_1$, $\\mathbf{A}_2$, and $\\mathbf{A}_3$ are smaller sub-matrices (blocks) that focus on local dependencies. Each block is a 4x4 matrix, and the zeros represent no direct attention between elements in different blocks.\n",
    "\n",
    "### Matrix Multiplication: Efficient vs. Inefficient\n",
    "\n",
    "#### Inefficient Matrix Multiplication\n",
    "\n",
    "In traditional (inefficient) matrix multiplication, we consider the entire matrix without leveraging its block structure. For two matrices $\\mathbf{A}$ and $\\mathbf{B}$, the multiplication $\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}$ requires:\n",
    "\n",
    "$\n",
    "C_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}\n",
    "$\n",
    "\n",
    "For a matrix of size $n \\times n$, this involves $O(n^3)$ operations.\n",
    "\n",
    "#### Efficient Matrix Multiplication\n",
    "\n",
    "In efficient matrix multiplication, we exploit the block structure. For our modular matrix $\\mathbf{A}$, we can multiply each block independently:\n",
    "\n",
    "$\n",
    "\\mathbf{C} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{A}_1 & 0 & 0 \\\\\n",
    "0 & \\mathbf{A}_2 & 0 \\\\\n",
    "0 & 0 & \\mathbf{A}_3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{B}_1 & 0 & 0 \\\\\n",
    "0 & \\mathbf{B}_2 & 0 \\\\\n",
    "0 & 0 & \\mathbf{B}_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{A}_1 \\cdot \\mathbf{B}_1 & 0 & 0 \\\\\n",
    "0 & \\mathbf{A}_2 \\cdot \\mathbf{B}_2 & 0 \\\\\n",
    "0 & 0 & \\mathbf{A}_3 \\cdot \\mathbf{B}_3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Here, each block multiplication $\\mathbf{A}_i \\cdot \\mathbf{B}_i$ involves $O(m^3)$ operations for block size $m \\times m$. If we have $k$ blocks, the total complexity becomes $O(k \\cdot m^3)$, where $k \\cdot m = n$. Therefore, the efficient method has a complexity of:\n",
    "\n",
    "$\n",
    "O\\left( \\frac{n}{m} \\cdot m^3 \\right) = O(n \\cdot m^2)\n",
    "$\n",
    "\n",
    "This is more efficient than the $O(n^3)$ complexity of the traditional method, especially for small $m$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61313dc",
   "metadata": {},
   "source": [
    "#### Reducing Memory Usage\n",
    "\n",
    "**Traditional Attention Mechanism**:\n",
    "- In traditional attention mechanisms, the computation of attention scores and weighted sums involves storing large intermediate matrices.\n",
    "- For a sequence of length $n$ and an embedding dimension $d$, we typically compute the attention scores as a matrix of size $n \\times n$, which can become very large for long sequences.\n",
    "- Additionally, storing all intermediate results for backpropagation during training further increases memory usage.\n",
    "\n",
    "**Flash Attention**:\n",
    "- Flash attention uses more efficient memory access patterns to minimize these large intermediate matrices.\n",
    "- Instead of storing the entire $n \\times n$ attention score matrix, it processes smaller chunks at a time, reducing the peak memory usage.\n",
    "- This chunking approach leverages temporal locality in memory access patterns, ensuring that the data needed next is already in the cache, thus reducing memory access time and usage.\n",
    "\n",
    "#### Speeding Up Computations\n",
    "\n",
    "**Traditional Attention Mechanism**:\n",
    "- Traditional attention mechanisms perform a lot of redundant computations, especially when dealing with long sequences.\n",
    "- The computation of attention scores involves multiple matrix multiplications and normalizations, which can be slow for large matrices.\n",
    "- These operations are often memory-bound, meaning the speed is limited by how fast data can be moved in and out of memory.\n",
    "\n",
    "**Flash Attention**:\n",
    "- Flash attention takes advantage of modern hardware capabilities, such as parallel processing units (GPUs or TPUs) and optimized memory access.\n",
    "- By breaking down the attention computation into smaller chunks, it allows for better parallelization, where multiple chunks can be processed simultaneously.\n",
    "- It uses optimized libraries and hardware instructions to speed up matrix multiplications and softmax computations, reducing the overall computation time.\n",
    "- The reduced memory usage also means less data movement, which further speeds up the computations.\n",
    "\n",
    "#### Maintaining Accuracy\n",
    "\n",
    "**Traditional Attention Mechanism**:\n",
    "- Accuracy in traditional attention mechanisms comes from the precise computation of attention scores and the weighted sum of value vectors.\n",
    "- Any optimization or approximation that alters these computations can potentially degrade performance.\n",
    "\n",
    "**Flash Attention**:\n",
    "- Flash attention ensures that despite optimizations, the fundamental computations remain the same.\n",
    "- The chunking approach used in flash attention does not approximate or alter the attention scores or the weighted sum; it simply reorganizes the computation to be more efficient.\n",
    "- By maintaining the integrity of these computations, flash attention preserves the accuracy of the attention mechanism.\n",
    "- Additionally, any numerical stability techniques (e.g., careful handling of floating-point operations) are incorporated to prevent accuracy loss during optimized computations.\n",
    "\n",
    "### Illustrative Example\n",
    "\n",
    "To better understand these points, let's consider a simple example of matrix multiplication, which is at the heart of attention mechanisms.\n",
    "\n",
    "**Traditional Approach**:\n",
    "- Multiply two large matrices directly, storing the entire result matrix in memory before proceeding to the next step.\n",
    "- This can quickly exhaust available memory and slow down computations due to frequent memory access.\n",
    "\n",
    "**Optimized Approach (Flash Attention)**:\n",
    "- Break down the matrices into smaller sub-matrices or tiles.\n",
    "- Multiply corresponding sub-matrices, store only necessary intermediate results, and combine the results at the end.\n",
    "- This reduces memory usage and takes advantage of parallel processing to speed up the multiplication.\n",
    "\n",
    "Hereâ€™s a simplified analogy:\n",
    "- Imagine you have a huge book to read, but you have a small table. Instead of trying to spread the entire book out on the table, you tear out one chapter at a time, read it, and then move on to the next. This way, you use your table space efficiently and read more quickly without getting overwhelmed.\n",
    "\n",
    "In conclusion, flash attention reduces memory usage by processing data in smaller chunks, speeds up computations by leveraging modern hardware capabilities and efficient memory access patterns, and maintains accuracy by ensuring the core computations remain unchanged. This makes it a highly practical and efficient optimization for attention mechanisms in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be4eda",
   "metadata": {},
   "source": [
    "## Chunking example\n",
    "\n",
    "To illustrate the concept of chunking within the code, you would need to implement an approach where the attention mechanism processes smaller subsections (chunks) of the sequence at a time. Here is a modified version of the code that explicitly demonstrates chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61eb61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FlashAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, chunk_size=32):\n",
    "        super(FlashAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.chunk_size = chunk_size  # Define the chunk size for splitting\n",
    "        \n",
    "        self.query_linear = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_linear = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_linear = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_linear = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        batch_size, seq_length, embed_dim = queries.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        queries = self.query_linear(queries).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        keys = self.key_linear(keys).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.value_linear(values).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        queries = queries.transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "        keys = keys.transpose(1, 2)        # (batch_size, num_heads, seq_length, head_dim)\n",
    "        values = values.transpose(1, 2)    # (batch_size, num_heads, seq_length, head_dim)\n",
    "        \n",
    "        attention_output = torch.zeros_like(values)\n",
    "        \n",
    "        for i in range(0, seq_length, self.chunk_size):\n",
    "            chunk_queries = queries[:, :, i:i+self.chunk_size]\n",
    "            chunk_keys = keys[:, :, i:i+self.chunk_size]\n",
    "            chunk_values = values[:, :, i:i+self.chunk_size]\n",
    "            \n",
    "            # Scaled dot-product attention for the chunk\n",
    "            scores = torch.matmul(chunk_queries, chunk_keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "            \n",
    "            if mask is not None:\n",
    "                chunk_mask = mask[:, :, i:i+self.chunk_size]\n",
    "                scores = scores.masked_fill(chunk_mask == 0, float('-inf'))\n",
    "            \n",
    "            attention_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            # Apply attention weights to values\n",
    "            chunk_output = torch.matmul(attention_weights, chunk_values)\n",
    "            \n",
    "            # Store the chunk output in the final output tensor\n",
    "            attention_output[:, :, i:i+self.chunk_size] = chunk_output\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.out_linear(attention_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "embed_dim = 64\n",
    "num_heads = 8\n",
    "seq_length = 128\n",
    "batch_size = 2\n",
    "chunk_size = 32  # Define chunk size\n",
    "\n",
    "queries = torch.randn(batch_size, seq_length, embed_dim)\n",
    "keys = torch.randn(batch_size, seq_length, embed_dim)\n",
    "values = torch.randn(batch_size, seq_length, embed_dim)\n",
    "\n",
    "flash_attention = FlashAttention(embed_dim, num_heads, chunk_size)\n",
    "output = flash_attention(queries, keys, values)\n",
    "\n",
    "print(output.shape)  # Expected output shape: (batch_size, seq_length, embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f282f",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "* Chunk Size Definition: The chunk_size parameter is introduced to control the size of each chunk.\n",
    "* Chunking Loop: The attention computation is performed inside a loop that processes chunks of the sequence (lines inside the for loop).\n",
    "* Chunk Processing: Each chunk of the queries, keys, and values is processed separately, and their outputs are stored in the final output tensor.\n",
    "\n",
    "This approach explicitly demonstrates how chunking can be used to reduce memory usage and improve computational efficiency in the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934526d",
   "metadata": {},
   "source": [
    "### Why Chunking Works in Flash Attention\n",
    "\n",
    "#### Modularity and Block-Diagonal Structure\n",
    "\n",
    "Attention matrices in many applications, particularly those involving long sequences, often exhibit a modular, block-diagonal structure. This means that attention scores are primarily focused around the main diagonal, reflecting the fact that elements in the sequence are more likely to interact with their nearby neighbors than with distant elements. This natural locality in the data is a key reason why chunking works effectively in flash attention mechanisms.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **Local Attention**:\n",
    "   - **Modularity**: Many tasks, such as natural language processing, involve strong local dependencies where elements interact mostly with their immediate neighbors. This creates a block-diagonal pattern in the attention matrix.\n",
    "   - **Efficiency**: Chunking leverages this locality by processing smaller blocks of the sequence independently, which aligns with the natural modularity in the data.\n",
    "\n",
    "2. **Memory Efficiency**:\n",
    "   - **Reduced Intermediate Storage**: Traditional attention mechanisms require storing large intermediate matrices for the entire sequence, which can quickly exhaust memory. Chunking reduces this memory usage by processing smaller sections of the sequence at a time.\n",
    "   - **Cache Optimization**: Smaller chunks fit better in the processor's cache, minimizing the time spent moving data between different levels of memory and improving computational speed.\n",
    "\n",
    "3. **Computational Efficiency**:\n",
    "   - **Parallel Processing**: By breaking the sequence into smaller chunks, the computations can be parallelized more effectively. Each chunk can be processed independently, making better use of parallel processing capabilities of modern hardware like GPUs.\n",
    "   - **Avoiding Redundancy**: Chunking focuses computations on smaller, relevant sections of the sequence, avoiding redundant calculations that would occur if processing the entire sequence at once.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- **Efficiency Gains**: By processing smaller, more manageable chunks, flash attention reduces memory usage and computation time, making it scalable for longer sequences.\n",
    "- **Maintaining Accuracy**: Chunking does not alter the fundamental computations of the attention mechanism but reorganizes them to be more efficient. This ensures that the accuracy of the attention mechanism is preserved.\n",
    "- **Real-World Applications**: This approach is particularly beneficial in real-world applications where sequences can be very long, such as in natural language processing, where sentences or documents can contain many words.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Chunking works effectively in flash attention mechanisms because it aligns with the modular, block-diagonal structure often found in attention matrices. By processing smaller chunks, flash attention optimizes memory usage, computational speed, and maintains accuracy, making it a highly practical optimization for handling long sequences in attention-based models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In many applications, especially those involving long sequences, attention matrices do exhibit modularity, often appearing as blocks along the main diagonal. This block structure reflects the fact that, in many practical tasks, attention is often local. For example, in natural language processing, the meaning of a word is typically influenced more by its surrounding words than by words far away in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f21738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def generate_block_diagonal_attention_matrix(seq_length, block_size):\n",
    "    attention_matrix = np.zeros((seq_length, seq_length))\n",
    "    for i in range(0, seq_length, block_size):\n",
    "        end = min(i + block_size, seq_length)\n",
    "        attention_matrix[i:end, i:end] = np.random.rand(end - i, end - i)\n",
    "    return attention_matrix\n",
    "\n",
    "seq_length = 100\n",
    "block_size = 10\n",
    "attention_matrix = generate_block_diagonal_attention_matrix(seq_length, block_size)\n",
    "\n",
    "plt.imshow(attention_matrix, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Block Diagonal Attention Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f1cef",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **Block Diagonal Matrix Generation**:\n",
    "   - The `generate_block_diagonal_attention_matrix` function creates a sequence-length by sequence-length matrix with blocks along the diagonal.\n",
    "   - Each block represents local attention within a window of `block_size`.\n",
    "\n",
    "2. **Visualization**:\n",
    "   - The `imshow` function from `matplotlib` visualizes the attention matrix, showing how attention is distributed locally in blocks.\n",
    "\n",
    "### Implications for Flash Attention\n",
    "\n",
    "- **Efficiency**: Leveraging the block structure in attention matrices can significantly reduce memory usage and computation time.\n",
    "- **Implementation**: Flash attention mechanisms can take advantage of this by focusing computations on the blocks rather than the entire matrix.\n",
    "- **Scalability**: This modular approach allows the attention mechanism to scale to longer sequences by ensuring that only relevant parts of the sequence are attended to, avoiding the quadratic complexity of traditional attention.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Attention matrices often exhibit a modular, block-diagonal structure, especially when local attention mechanisms are used. This structure can be exploited by flash attention to improve efficiency and scalability without sacrificing accuracy. Understanding and leveraging this modularity is key to optimizing attention mechanisms for practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4eb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
